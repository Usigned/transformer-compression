{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import rl_utils\n",
    "import args\n",
    "\n",
    "from env import Env\n",
    "from ddpg import DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/500 [00:00<?, ?it/s]c:\\Users\\1\\Desktop\\design-code\\ddpg.py:69: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
      "INFO:root:**************************** start to evaluate *******************************\n",
      "Iteration:   4%|▍         | 12/313 [02:21<59:06, 11.78s/it]\n",
      "Iteration 0:   0%|          | 0/500 [02:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     17\u001b[0m action_dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     18\u001b[0m agent \u001b[39m=\u001b[39m DDPG(\n\u001b[0;32m     19\u001b[0m     state_dim\u001b[39m=\u001b[39mstate_dim,\n\u001b[0;32m     20\u001b[0m     action_dim\u001b[39m=\u001b[39maction_dim, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs\u001b[39m.\u001b[39mAGENT \n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m return_list, best_reward, best_s, r, s  \u001b[39m=\u001b[39m rl_utils\u001b[39m.\u001b[39;49mtrain_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size)\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\rl_utils.py:71\u001b[0m, in \u001b[0;36mtrain_off_policy_agent\u001b[1;34m(env, agent, num_episodes, replay_buffer, minimal_size, batch_size)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     70\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtake_action(state, num_episodes\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m i \u001b[39m+\u001b[39m i_episode\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     72\u001b[0m     \u001b[39m# replay_buffer.add(state, action, reward, next_state, done)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     T\u001b[39m.\u001b[39mappend((state, action, reward, next_state, done))\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\env.py:199\u001b[0m, in \u001b[0;36mEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adjust_strategy()\n\u001b[0;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_strategy()\n\u001b[1;32m--> 199\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward()\n\u001b[0;32m    200\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    202\u001b[0m info_set[\u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m finish\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mquant_pi:\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_strategy\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mprune_pi:\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprune_strategy\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mreward: \u001b[39m\u001b[39m{\u001b[39;00mreward\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\env.py:249\u001b[0m, in \u001b[0;36mEnv.reward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     finetune(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 249\u001b[0m acc \u001b[39m=\u001b[39m eval_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtestloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    250\u001b[0m \u001b[39mreturn\u001b[39;00m (acc \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mori_acc) \u001b[39m*\u001b[39m \u001b[39m0.1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\train.py:89\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m     87\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(t\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m batch)\n\u001b[0;32m     88\u001b[0m batch_X, batch_Y \u001b[39m=\u001b[39m batch\n\u001b[1;32m---> 89\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_X)\n\u001b[0;32m     90\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m     91\u001b[0m total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_Y\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\model.py:236\u001b[0m, in \u001b[0;36mCAFIA_Transformer.forward\u001b[1;34m(self, batch_X)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batch_X):\n\u001b[1;32m--> 236\u001b[0m     feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvit(batch_X)\n\u001b[0;32m    237\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(feat[:, \u001b[39m0\u001b[39m])\n\u001b[0;32m    238\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\model.py:208\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    206\u001b[0m emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([cls_token, emb], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    207\u001b[0m \u001b[39m# transformer\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(emb)\n\u001b[0;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m feat\n",
      "File \u001b[1;32md:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\model.py:159\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    157\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding(x)\n\u001b[0;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers:\n\u001b[1;32m--> 159\u001b[0m     out \u001b[39m=\u001b[39m layer(out)\n\u001b[0;32m    160\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(out)\n\u001b[0;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\model.py:137\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    135\u001b[0m residual \u001b[39m=\u001b[39m out\n\u001b[0;32m    136\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(out)\n\u001b[1;32m--> 137\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(out)\n\u001b[0;32m    138\u001b[0m out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m residual\n\u001b[0;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\model.py:59\u001b[0m, in \u001b[0;36mMlpBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1:\n\u001b[0;32m     58\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(out)\n\u001b[1;32m---> 59\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(out)\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2:\n\u001b[0;32m     61\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(out)\n",
      "File \u001b[1;32md:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\quant_utils.py:306\u001b[0m, in \u001b[0;36mQLinear.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m--> 306\u001b[0m     inputs, weight, bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quantize(\n\u001b[0;32m    307\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, bias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(inputs, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\quant_utils.py:263\u001b[0m, in \u001b[0;36mQModule._quantize\u001b[1;34m(self, inputs, weight, bias)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_quantize\u001b[39m(\u001b[39mself\u001b[39m, inputs, weight, bias):\n\u001b[1;32m--> 263\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quantize_activation(inputs\u001b[39m=\u001b[39;49minputs)\n\u001b[0;32m    264\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_quantize_weight(weight\u001b[39m=\u001b[39mweight)\n\u001b[0;32m    265\u001b[0m     \u001b[39m# bias = self._quantize_bias(bias=bias)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1\\Desktop\\design-code\\quant_utils.py:150\u001b[0m, in \u001b[0;36mQModule._quantize_activation\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    148\u001b[0m ori_x \u001b[39m=\u001b[39m inputs\n\u001b[0;32m    149\u001b[0m scaling_factor \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m (\u001b[39m2.0\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_a_bit \u001b[39m-\u001b[39m \u001b[39m1.0\u001b[39m)\n\u001b[1;32m--> 150\u001b[0m x \u001b[39m=\u001b[39m ori_x\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mclone()\n\u001b[0;32m    151\u001b[0m x\u001b[39m.\u001b[39mdiv_(scaling_factor)\u001b[39m.\u001b[39mround_()\u001b[39m.\u001b[39mmul_(scaling_factor)\n\u001b[0;32m    153\u001b[0m \u001b[39mreturn\u001b[39;00m STE\u001b[39m.\u001b[39mapply(ori_x, x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import get_vit\n",
    "\n",
    "\n",
    "num_episodes = 5000\n",
    "minimal_size = 1000\n",
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "\n",
    "env_name = 'MyEnv'\n",
    "env = Env(get_vit(args.MQVIT), **args.ENV)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "replay_buffer = rl_utils.ReplayBuffer(buffer_size)\n",
    "state_dim = env.state_dim\n",
    "action_dim = 1\n",
    "agent = DDPG(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim, **args.AGENT \n",
    ")\n",
    "\n",
    "return_list, best_reward, best_s, r, s  = rl_utils.train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('DDPG on {}'.format(env_name))\n",
    "plt.show()\n",
    "\n",
    "mv_return = rl_utils.moving_average(return_list, 9)\n",
    "plt.plot(episodes_list, mv_return)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('DDPG on {}'.format(env_name))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
