{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import get_vit\n",
    "import args\n",
    "from data import get_cifar10_dataloader\n",
    "from train import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['vit.transformer.encoder_layers.0.attn.query.activation_range', 'vit.transformer.encoder_layers.0.attn.query.weight_range', 'vit.transformer.encoder_layers.0.attn.key.activation_range', 'vit.transformer.encoder_layers.0.attn.key.weight_range', 'vit.transformer.encoder_layers.0.attn.value.activation_range', 'vit.transformer.encoder_layers.0.attn.value.weight_range', 'vit.transformer.encoder_layers.0.attn.out.activation_range', 'vit.transformer.encoder_layers.0.attn.out.weight_range', 'vit.transformer.encoder_layers.0.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.0.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.0.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.0.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.1.attn.query.activation_range', 'vit.transformer.encoder_layers.1.attn.query.weight_range', 'vit.transformer.encoder_layers.1.attn.key.activation_range', 'vit.transformer.encoder_layers.1.attn.key.weight_range', 'vit.transformer.encoder_layers.1.attn.value.activation_range', 'vit.transformer.encoder_layers.1.attn.value.weight_range', 'vit.transformer.encoder_layers.1.attn.out.activation_range', 'vit.transformer.encoder_layers.1.attn.out.weight_range', 'vit.transformer.encoder_layers.1.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.1.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.1.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.1.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.2.attn.query.activation_range', 'vit.transformer.encoder_layers.2.attn.query.weight_range', 'vit.transformer.encoder_layers.2.attn.key.activation_range', 'vit.transformer.encoder_layers.2.attn.key.weight_range', 'vit.transformer.encoder_layers.2.attn.value.activation_range', 'vit.transformer.encoder_layers.2.attn.value.weight_range', 'vit.transformer.encoder_layers.2.attn.out.activation_range', 'vit.transformer.encoder_layers.2.attn.out.weight_range', 'vit.transformer.encoder_layers.2.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.2.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.2.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.2.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.3.attn.query.activation_range', 'vit.transformer.encoder_layers.3.attn.query.weight_range', 'vit.transformer.encoder_layers.3.attn.key.activation_range', 'vit.transformer.encoder_layers.3.attn.key.weight_range', 'vit.transformer.encoder_layers.3.attn.value.activation_range', 'vit.transformer.encoder_layers.3.attn.value.weight_range', 'vit.transformer.encoder_layers.3.attn.out.activation_range', 'vit.transformer.encoder_layers.3.attn.out.weight_range', 'vit.transformer.encoder_layers.3.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.3.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.3.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.3.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.4.attn.query.activation_range', 'vit.transformer.encoder_layers.4.attn.query.weight_range', 'vit.transformer.encoder_layers.4.attn.key.activation_range', 'vit.transformer.encoder_layers.4.attn.key.weight_range', 'vit.transformer.encoder_layers.4.attn.value.activation_range', 'vit.transformer.encoder_layers.4.attn.value.weight_range', 'vit.transformer.encoder_layers.4.attn.out.activation_range', 'vit.transformer.encoder_layers.4.attn.out.weight_range', 'vit.transformer.encoder_layers.4.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.4.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.4.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.4.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.5.attn.query.activation_range', 'vit.transformer.encoder_layers.5.attn.query.weight_range', 'vit.transformer.encoder_layers.5.attn.key.activation_range', 'vit.transformer.encoder_layers.5.attn.key.weight_range', 'vit.transformer.encoder_layers.5.attn.value.activation_range', 'vit.transformer.encoder_layers.5.attn.value.weight_range', 'vit.transformer.encoder_layers.5.attn.out.activation_range', 'vit.transformer.encoder_layers.5.attn.out.weight_range', 'vit.transformer.encoder_layers.5.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.5.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.5.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.5.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.6.attn.query.activation_range', 'vit.transformer.encoder_layers.6.attn.query.weight_range', 'vit.transformer.encoder_layers.6.attn.key.activation_range', 'vit.transformer.encoder_layers.6.attn.key.weight_range', 'vit.transformer.encoder_layers.6.attn.value.activation_range', 'vit.transformer.encoder_layers.6.attn.value.weight_range', 'vit.transformer.encoder_layers.6.attn.out.activation_range', 'vit.transformer.encoder_layers.6.attn.out.weight_range', 'vit.transformer.encoder_layers.6.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.6.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.6.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.6.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.7.attn.query.activation_range', 'vit.transformer.encoder_layers.7.attn.query.weight_range', 'vit.transformer.encoder_layers.7.attn.key.activation_range', 'vit.transformer.encoder_layers.7.attn.key.weight_range', 'vit.transformer.encoder_layers.7.attn.value.activation_range', 'vit.transformer.encoder_layers.7.attn.value.weight_range', 'vit.transformer.encoder_layers.7.attn.out.activation_range', 'vit.transformer.encoder_layers.7.attn.out.weight_range', 'vit.transformer.encoder_layers.7.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.7.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.7.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.7.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.8.attn.query.activation_range', 'vit.transformer.encoder_layers.8.attn.query.weight_range', 'vit.transformer.encoder_layers.8.attn.key.activation_range', 'vit.transformer.encoder_layers.8.attn.key.weight_range', 'vit.transformer.encoder_layers.8.attn.value.activation_range', 'vit.transformer.encoder_layers.8.attn.value.weight_range', 'vit.transformer.encoder_layers.8.attn.out.activation_range', 'vit.transformer.encoder_layers.8.attn.out.weight_range', 'vit.transformer.encoder_layers.8.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.8.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.8.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.8.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.9.attn.query.activation_range', 'vit.transformer.encoder_layers.9.attn.query.weight_range', 'vit.transformer.encoder_layers.9.attn.key.activation_range', 'vit.transformer.encoder_layers.9.attn.key.weight_range', 'vit.transformer.encoder_layers.9.attn.value.activation_range', 'vit.transformer.encoder_layers.9.attn.value.weight_range', 'vit.transformer.encoder_layers.9.attn.out.activation_range', 'vit.transformer.encoder_layers.9.attn.out.weight_range', 'vit.transformer.encoder_layers.9.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.9.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.9.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.9.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.10.attn.query.activation_range', 'vit.transformer.encoder_layers.10.attn.query.weight_range', 'vit.transformer.encoder_layers.10.attn.key.activation_range', 'vit.transformer.encoder_layers.10.attn.key.weight_range', 'vit.transformer.encoder_layers.10.attn.value.activation_range', 'vit.transformer.encoder_layers.10.attn.value.weight_range', 'vit.transformer.encoder_layers.10.attn.out.activation_range', 'vit.transformer.encoder_layers.10.attn.out.weight_range', 'vit.transformer.encoder_layers.10.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.10.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.10.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.10.mlp.fc2.weight_range', 'vit.transformer.encoder_layers.11.attn.query.activation_range', 'vit.transformer.encoder_layers.11.attn.query.weight_range', 'vit.transformer.encoder_layers.11.attn.key.activation_range', 'vit.transformer.encoder_layers.11.attn.key.weight_range', 'vit.transformer.encoder_layers.11.attn.value.activation_range', 'vit.transformer.encoder_layers.11.attn.value.weight_range', 'vit.transformer.encoder_layers.11.attn.out.activation_range', 'vit.transformer.encoder_layers.11.attn.out.weight_range', 'vit.transformer.encoder_layers.11.mlp.fc1.activation_range', 'vit.transformer.encoder_layers.11.mlp.fc1.weight_range', 'vit.transformer.encoder_layers.11.mlp.fc2.activation_range', 'vit.transformer.encoder_layers.11.mlp.fc2.weight_range'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "path = r'D:\\d-storage\\output\\pat-0.5.pt'\n",
    "trainloader = get_cifar10_dataloader(train=True)\n",
    "testloader = get_cifar10_dataloader(train=False)\n",
    "mqvit = get_vit(args.MQVIT, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmsa import mix_prune, get_mask_idx\n",
    "from quant_utils import set_mixed_precision, get_quantizable_idx, mix_weight_act_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_idx = get_quantizable_idx(mqvit)\n",
    "set_mixed_precision(mqvit, q_idx, strategy=mix_weight_act_strategy([8]*len(q_idx), [8]*len(q_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_idx= get_mask_idx(mqvit)\n",
    "mix_prune(mqvit, p_idx, [.75]*len(p_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAFIA_Transformer(\n",
       "  (vit): VisionTransformer(\n",
       "    (embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (transformer): Encoder(\n",
       "      (pos_embedding): PositionEmbs(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder_layers): ModuleList(\n",
       "        (0): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): EncoderBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MaskedSelfAttention(\n",
       "            (query): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (key): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (value): QLinearGeneral(in_dim=(768,), feat_dim=(12, 64), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (out): QLinearGeneral(in_dim=(12, 64), feat_dim=(768,), w_bit=8, a_bit=8, half wave, tensor wise)\n",
       "            (mask): LearnableMask(p=1.0, fixed=True, pruned dim=3)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MlpBlock(\n",
       "            (fc1): QLinear(in_features=768, out_features=3072, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (fc2): QLinear(in_features=3072, out_features=768, bias=True, w_bit=8, a_bit=8, half wave)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqvit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
